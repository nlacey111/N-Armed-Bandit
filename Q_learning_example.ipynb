{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using q learning on my custom environment (two armed bandit) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "import gymnasium_env_Bandit\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing what needs to be initialized: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nicla\\.miniconda3\\envs\\torch\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:197: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.deprecation(\n",
      "c:\\Users\\nicla\\.miniconda3\\envs\\torch\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:210: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.deprecation(\n",
      "c:\\Users\\nicla\\.miniconda3\\envs\\torch\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:164: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: int32\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\nicla\\.miniconda3\\envs\\torch\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:188: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    }
   ],
   "source": [
    "env = gymnasium.make('gymnasium_env_Bandit/Bandit-v0')\n",
    "new_state, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for two armed bandit\n",
    "learning_rate = 0.01\n",
    "discount_factor = 0.9\n",
    "episodes = 1000\n",
    "obsspace = env.observation_space\n",
    "#print(obsspace)\n",
    "discrete_os_size = 1\n",
    "#iteration = 1\n",
    "\n",
    "# how to get the iteration number from the environment -> env.iteration\n",
    "\n",
    "q_table = np.zeros((discrete_os_size, env.action_space.n)) #should only have two options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0.]]\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(q_table)\n",
    "print(q_table[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make loop for episodes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(episodes):\n",
    "    new_state, _ = env.reset()\n",
    "    \n",
    "    done = False\n",
    "    while not done:\n",
    "        action = np.argmax(q_table)  # Choose the best action, returns 0 or 1 \n",
    "        new_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        #print(new_state)\n",
    "        \n",
    "        if not done:\n",
    "            max_future_q = np.max(q_table)  # The best future reward. should this be max or argmax??\n",
    "            current_q = q_table[0, action] \n",
    "            new_q = (1 - learning_rate) * current_q + learning_rate * (reward + discount_factor * max_future_q)\n",
    "            q_table[0, action] = new_q\n",
    "            #print(env.iteration)\n",
    "            \n",
    "        elif env.iteration == 200: \n",
    "            print(f\"We made it on episode {episode}\")\n",
    "            q_table[0, action] = 0\n",
    "        \n",
    "                  \n",
    "env.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph which action was chosen over the episodes. \n",
    "graph cumulative regret\n",
    "\n",
    "\n",
    "how to change things set in the environment- the probabilities of the bandits eg. can i just save or do i need to reregister the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q table:  [[4.55914972 0.        ]]\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.55"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"q table: \", q_table)\n",
    "print(np.argmax(q_table))\n",
    "np.random.rand()\n",
    "\n",
    "\n",
    "env.machine_probs[1]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
